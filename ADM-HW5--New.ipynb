{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on wikigraph.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the reduced csv file containing for each single edge in the graph the starting node (Named Source) and the final node (Named Target). To obtain a correct display we used a separator with respect to the tab and subsequently we renamed the two columns 0,1 as Source and Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>1185516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>1059989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108</td>\n",
       "      <td>1062426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108</td>\n",
       "      <td>1161925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>541222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source   Target\n",
       "0      95  1185516\n",
       "1     108  1059989\n",
       "2     108  1062426\n",
       "3     108  1161925\n",
       "4     134   541222"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = pd.read_csv(\"wikigraph_reduced.csv\", sep='\\t', usecols =[\"0\",\"1\"]) #Read file csv with separator \\t\n",
    "graph.columns = [\"Source\",\"Target\"] #Rename our columns\n",
    "graph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on page names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have uploaded the Text file containing the names of each single page. Once loaded, to obtain a correct view, we wanted to split the single column containing both the name of the page and the node associated with the page into two separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Node</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chiasmal syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kleroterion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pinakion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LyndonHochschildSerre spectral sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zariski's main theorem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Pages\n",
       "Node                                         \n",
       "0                           Chiasmal syndrome\n",
       "1                                 Kleroterion\n",
       "2                                    Pinakion\n",
       "3     LyndonHochschildSerre spectral sequence\n",
       "4                      Zariski's main theorem"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name = pd.read_csv(\"Names.txt\",sep=\"\\t\", names = [\"Page Names\"]) #Read file csv with separator \\t\n",
    "node =[] #Create a empty list for node\n",
    "page= [] ##Create a empty list for node\n",
    "for i in (data_name[\"Page Names\"]): #We extract in each row from single column node and page\n",
    "    node.append(i.split()[0]) #Split the single column and take the first element (NODE)\n",
    "    page.append(\" \".join(i.split()[1:])) #From the second element we have the name of page\n",
    "data_name[\"Node\"] = node #Create new column for Node\n",
    "data_name[\"Pages\"] = page #Create new column for pages\n",
    "data_name = data_name.drop([\"Page Names\"], axis=1).set_index(\"Node\") #Drop our original column and set our index in colun node\n",
    "data_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on topcast categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we uploaded the text file containing the list of pages associated with this category for each single category. Also in this case for a correct visualization we have carried out various operations. Once you have read the text file with separator and division into columns. We then worked on the two columns, on the Category column we deleted the initial part of the string for each row while for the PageList column we split the pages and created a list. We have also only considered categories with a number of articles between 5000 and 30000 as required by the Homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>PageList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English_footballers</td>\n",
       "      <td>[22860, 28411, 28961, 28979, 29264, 29573, 295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The_Football_League_players</td>\n",
       "      <td>[14003, 23536, 27109, 27348, 27459, 27989, 280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Association_football_forwards</td>\n",
       "      <td>[26876, 26877, 26879, 26887, 26892, 26904, 269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Association_football_midfielders</td>\n",
       "      <td>[14003, 15291, 23536, 26880, 26882, 26885, 268...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Association_football_defenders</td>\n",
       "      <td>[15217, 22860, 26873, 26878, 26881, 26898, 269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harvard_University_alumni</td>\n",
       "      <td>[77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Major_League_Baseball_pitchers</td>\n",
       "      <td>[79, 24213, 33054, 37167, 53973, 63107, 69823,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Members_of_the_United_Kingdom_Parliament_for_E...</td>\n",
       "      <td>[29098, 29493, 29585, 30255, 30389, 30505, 306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian_films</td>\n",
       "      <td>[1308, 29286, 53565, 70274, 70275, 70797, 1233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>[98, 126, 227, 1823, 2170, 2223, 13215, 14003,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rivers_of_Romani</td>\n",
       "      <td>[72111, 72112, 72113, 72114, 72115, 72116, 721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Main_Belt_asteroids</td>\n",
       "      <td>[13184, 13992, 22277, 23539, 29163, 33254, 334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Asteroids_named_for_people</td>\n",
       "      <td>[13184, 13992, 23539, 33990, 33991, 37535, 375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>English-language_films</td>\n",
       "      <td>[55, 134, 153, 214, 1083, 1084, 1087, 1089, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>American_films</td>\n",
       "      <td>[134, 153, 173, 1083, 1087, 1089, 1131, 1152, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>[174, 938, 980, 1086, 1088, 1099, 1106, 1109, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>American_film_actors</td>\n",
       "      <td>[174, 980, 1088, 1099, 1106, 1109, 1121, 1122,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Debut_albums</td>\n",
       "      <td>[95, 108, 112, 113, 190, 254, 1413, 1451, 1452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Black-and-white_films</td>\n",
       "      <td>[134, 16130, 19763, 26846, 31864, 35833, 38917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Year_of_birth_missing</td>\n",
       "      <td>[147, 171, 1056, 1656, 13215, 14003, 15293, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "      <td>[167, 1100, 1104, 1118, 1119, 1139, 1140, 1144...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category  \\\n",
       "0                                 English_footballers   \n",
       "1                         The_Football_League_players   \n",
       "2                       Association_football_forwards   \n",
       "3                    Association_football_midfielders   \n",
       "4                      Association_football_defenders   \n",
       "5                           Harvard_University_alumni   \n",
       "6                      Major_League_Baseball_pitchers   \n",
       "7   Members_of_the_United_Kingdom_Parliament_for_E...   \n",
       "8                                        Indian_films   \n",
       "9                               Year_of_death_missing   \n",
       "10                                   Rivers_of_Romani   \n",
       "11                                Main_Belt_asteroids   \n",
       "12                         Asteroids_named_for_people   \n",
       "13                             English-language_films   \n",
       "14                                     American_films   \n",
       "15                         American_television_actors   \n",
       "16                               American_film_actors   \n",
       "17                                       Debut_albums   \n",
       "18                              Black-and-white_films   \n",
       "19                              Year_of_birth_missing   \n",
       "20             Place_of_birth_missing_(living_people)   \n",
       "\n",
       "                                             PageList  \n",
       "0   [22860, 28411, 28961, 28979, 29264, 29573, 295...  \n",
       "1   [14003, 23536, 27109, 27348, 27459, 27989, 280...  \n",
       "2   [26876, 26877, 26879, 26887, 26892, 26904, 269...  \n",
       "3   [14003, 15291, 23536, 26880, 26882, 26885, 268...  \n",
       "4   [15217, 22860, 26873, 26878, 26881, 26898, 269...  \n",
       "5   [77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...  \n",
       "6   [79, 24213, 33054, 37167, 53973, 63107, 69823,...  \n",
       "7   [29098, 29493, 29585, 30255, 30389, 30505, 306...  \n",
       "8   [1308, 29286, 53565, 70274, 70275, 70797, 1233...  \n",
       "9   [98, 126, 227, 1823, 2170, 2223, 13215, 14003,...  \n",
       "10  [72111, 72112, 72113, 72114, 72115, 72116, 721...  \n",
       "11  [13184, 13992, 22277, 23539, 29163, 33254, 334...  \n",
       "12  [13184, 13992, 23539, 33990, 33991, 37535, 375...  \n",
       "13  [55, 134, 153, 214, 1083, 1084, 1087, 1089, 11...  \n",
       "14  [134, 153, 173, 1083, 1087, 1089, 1131, 1152, ...  \n",
       "15  [174, 938, 980, 1086, 1088, 1099, 1106, 1109, ...  \n",
       "16  [174, 980, 1088, 1099, 1106, 1109, 1121, 1122,...  \n",
       "17  [95, 108, 112, 113, 190, 254, 1413, 1451, 1452...  \n",
       "18  [134, 16130, 19763, 26846, 31864, 35833, 38917...  \n",
       "19  [147, 171, 1056, 1656, 13215, 14003, 15293, 22...  \n",
       "20  [167, 1100, 1104, 1118, 1119, 1139, 1140, 1144...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat = pd.read_csv('Categories.txt',sep=';', names = [\"Category\",\"PageList\"]) #Read file csv with separator \";\"\n",
    "data_cat['Category'] = data_cat['Category'].map(lambda x: x.lstrip('Category:').rstrip('aAbBcC')) #modify the Category column by removing the initial part of the string \"Category:\" for each row\n",
    "data_cat['PageList'] = data_cat['PageList'].map(lambda x: x.split()) #modify the PageList column by splitting our string of pageList in a list\n",
    "data_cat = data_cat[(data_cat['PageList'].map(len) >= 5000) & (data_cat['PageList'].map(len) <= 30000)].reset_index() #Remove all Category with a number of Page lower than 5000 and higher than 30000\n",
    "data_cat = data_cat.drop(\"index\", axis = 1)\n",
    "data_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to create a new data frame containing the category of belonging for each article. Subsequently we created a dictionary containing the individual articles as keys and the categories of belonging as values for each key. We did it all because one article might belong to a single category or multiple ones. In the case of multiple appearance, we have to break the ties uniformly at random. We have in fact modified the dictionary created by randomly choosing the category it belongs to for each article. Subsequently we obtained the reverse dictionary by choosing the various categories as keys and the list of pages associated with them as values for each key and finally we obtained a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = pd.concat([pd.DataFrame(data = {'Articles': data_cat.loc[i].PageList, \n",
    "                                             'Category': data_cat.loc[i].Category}) for i in data_cat.index], \n",
    "                       ignore_index=True) #Create a new datagrame where for each article we report the specific category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22860</td>\n",
       "      <td>English_footballers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28411</td>\n",
       "      <td>English_footballers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28961</td>\n",
       "      <td>English_footballers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28979</td>\n",
       "      <td>English_footballers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29264</td>\n",
       "      <td>English_footballers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198953</th>\n",
       "      <td>1790533</td>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198954</th>\n",
       "      <td>1790602</td>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198955</th>\n",
       "      <td>1790605</td>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198956</th>\n",
       "      <td>1790616</td>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198957</th>\n",
       "      <td>1790784</td>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Articles                                Category\n",
       "0         22860                     English_footballers\n",
       "1         28411                     English_footballers\n",
       "2         28961                     English_footballers\n",
       "3         28979                     English_footballers\n",
       "4         29264                     English_footballers\n",
       "...         ...                                     ...\n",
       "198953  1790533  Place_of_birth_missing_(living_people)\n",
       "198954  1790602  Place_of_birth_missing_(living_people)\n",
       "198955  1790605  Place_of_birth_missing_(living_people)\n",
       "198956  1790616  Place_of_birth_missing_(living_people)\n",
       "198957  1790784  Place_of_birth_missing_(living_people)\n",
       "\n",
       "[198958 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 198958/198958 [00:08<00:00, 23476.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dictionary = {} #Empty dictionary\n",
    "for i in tqdm(range(len(df_article))): #For each row in df_article dataframe\n",
    "    if df_article.loc[i,\"Articles\"] not in dictionary: #If Article not as dictionary key\n",
    "        dictionary[df_article.loc[i,\"Articles\"]] = [] #Put this article like key and create for this key create an empty list\n",
    "    dictionary[df_article.loc[i,\"Articles\"]].append(df_article.loc[i,\"Category\"])#Append for each key (Article) list of category where we found this Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "for keys in dictionary: #For each key in dictionary\n",
    "    new_value = random.choice(dictionary[keys]) #We decide with random a value of each keys and save this value like new_value\n",
    "    del dictionary[keys][:] #Remove all element in a list linked to specific keys\n",
    "    dictionary[keys] = new_value #Put for this key the new random_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_dict = {} #New dictionary to put like keys the category and for each category define all Articles\n",
    "for key, value in dictionary.items(): #For each tuple in \"dictionary\" containing key and value\n",
    "    reversed_dict.setdefault(value, []) #New key of reversed_dict equal to value in dictionary\n",
    "    reversed_dict[value].append(key) #Append value for each key in reversed_dict that are keys in \"dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dat = pd.DataFrame(reversed_dict.items(), columns=['Category', 'PageList']) #Obtain dataframe from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>PageList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Association_football_defenders</td>\n",
       "      <td>[22860, 29264, 72534, 72628, 72629, 72820, 729...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Year_of_birth_missing</td>\n",
       "      <td>[28411, 75577, 75740, 76100, 76134, 92857, 928...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English_footballers</td>\n",
       "      <td>[28961, 28979, 30896, 31902, 34042, 41141, 487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The_Football_League_players</td>\n",
       "      <td>[29573, 29582, 33973, 43573, 48582, 48583, 489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Association_football_midfielders</td>\n",
       "      <td>[30900, 72539, 72567, 72580, 73109, 73132, 731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Association_football_forwards</td>\n",
       "      <td>[48714, 72536, 72546, 72622, 72806, 73136, 731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>[72518, 72583, 72591, 72609, 72664, 72829, 728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "      <td>[75430, 79909, 84217, 84470, 526322, 75195, 96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Members_of_the_United_Kingdom_Parliament_for_E...</td>\n",
       "      <td>[535217, 536702, 536701, 539130, 539583, 53967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Harvard_University_alumni</td>\n",
       "      <td>[77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>[154133, 301418, 400707, 419812, 469376, 60207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>American_film_actors</td>\n",
       "      <td>[744125, 1054490, 1059808, 1062012, 1062673, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Major_League_Baseball_pitchers</td>\n",
       "      <td>[79, 24213, 33054, 37167, 53973, 63107, 69823,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indian_films</td>\n",
       "      <td>[1308, 29286, 53565, 70274, 70275, 70797, 1233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>English-language_films</td>\n",
       "      <td>[136155, 581089, 581926, 582063, 582197, 58247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Black-and-white_films</td>\n",
       "      <td>[356570, 587697, 588219, 588889, 589113, 58916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>American_films</td>\n",
       "      <td>[587405, 1083, 1087, 1089, 1178, 1267, 2247, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rivers_of_Romani</td>\n",
       "      <td>[72111, 72112, 72113, 72114, 72115, 72116, 721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Asteroids_named_for_people</td>\n",
       "      <td>[13184, 23539, 96574, 96577, 96579, 96580, 965...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Main_Belt_asteroids</td>\n",
       "      <td>[13992, 22277, 29163, 33254, 33401, 33791, 339...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Debut_albums</td>\n",
       "      <td>[95, 108, 112, 113, 190, 254, 1413, 1451, 1452...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category  \\\n",
       "0                      Association_football_defenders   \n",
       "1                               Year_of_birth_missing   \n",
       "2                                 English_footballers   \n",
       "3                         The_Football_League_players   \n",
       "4                    Association_football_midfielders   \n",
       "5                       Association_football_forwards   \n",
       "6                               Year_of_death_missing   \n",
       "7              Place_of_birth_missing_(living_people)   \n",
       "8   Members_of_the_United_Kingdom_Parliament_for_E...   \n",
       "9                           Harvard_University_alumni   \n",
       "10                         American_television_actors   \n",
       "11                               American_film_actors   \n",
       "12                     Major_League_Baseball_pitchers   \n",
       "13                                       Indian_films   \n",
       "14                             English-language_films   \n",
       "15                              Black-and-white_films   \n",
       "16                                     American_films   \n",
       "17                                   Rivers_of_Romani   \n",
       "18                         Asteroids_named_for_people   \n",
       "19                                Main_Belt_asteroids   \n",
       "20                                       Debut_albums   \n",
       "\n",
       "                                             PageList  \n",
       "0   [22860, 29264, 72534, 72628, 72629, 72820, 729...  \n",
       "1   [28411, 75577, 75740, 76100, 76134, 92857, 928...  \n",
       "2   [28961, 28979, 30896, 31902, 34042, 41141, 487...  \n",
       "3   [29573, 29582, 33973, 43573, 48582, 48583, 489...  \n",
       "4   [30900, 72539, 72567, 72580, 73109, 73132, 731...  \n",
       "5   [48714, 72536, 72546, 72622, 72806, 73136, 731...  \n",
       "6   [72518, 72583, 72591, 72609, 72664, 72829, 728...  \n",
       "7   [75430, 79909, 84217, 84470, 526322, 75195, 96...  \n",
       "8   [535217, 536702, 536701, 539130, 539583, 53967...  \n",
       "9   [77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...  \n",
       "10  [154133, 301418, 400707, 419812, 469376, 60207...  \n",
       "11  [744125, 1054490, 1059808, 1062012, 1062673, 1...  \n",
       "12  [79, 24213, 33054, 37167, 53973, 63107, 69823,...  \n",
       "13  [1308, 29286, 53565, 70274, 70275, 70797, 1233...  \n",
       "14  [136155, 581089, 581926, 582063, 582197, 58247...  \n",
       "15  [356570, 587697, 588219, 588889, 589113, 58916...  \n",
       "16  [587405, 1083, 1087, 1089, 1178, 1267, 2247, 1...  \n",
       "17  [72111, 72112, 72113, 72114, 72115, 72116, 721...  \n",
       "18  [13184, 23539, 96574, 96577, 96579, 96580, 965...  \n",
       "19  [13992, 22277, 29163, 33254, 33401, 33791, 339...  \n",
       "20  [95, 108, 112, 113, 190, 254, 1413, 1451, 1452...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Graph\n",
    "As a first step we can check if the graph is directed or not.\n",
    "### Is the graph directed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We create a new dataframe where the columns of 'graph' are inverted in order to create the list of edge for both of the dataframe and check in they are equal. In case of a positive answer we can say that the graph is not directed because for each edge from node _a_ to node _b_ there is an edge from node _b_ to node _a_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185516</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1059989</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1062426</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1161925</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>541222</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Target  Source\n",
       "0  1185516      95\n",
       "1  1059989     108\n",
       "2  1062426     108\n",
       "3  1161925     108\n",
       "4   541222     134"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the inverted graph\n",
    "graph_ = graph.copy()\n",
    "graph_inverted = graph_[graph_.columns[::-1]] \n",
    "graph_inverted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dataframes in a list of edges\n",
    "set_1 = graph.values.tolist()\n",
    "set_2 = graph_inverted.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph is directed\n"
     ]
    }
   ],
   "source": [
    "#calculating the intersection\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "intersect = intersection(set_1,set_2)\n",
    "if (intersect == set_1): # is the intersection equal to list of edge?\n",
    "    print('The graph is not directed')\n",
    "else:\n",
    "    print('The graph is directed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is directed since there exists an edge from a certain node _a_ to a node _b_ but not from _b_ to _a_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After figuring out if the graph was direct or not, we have created an empty Direct graph through the networkx package using as nodes all the elements belonging to the Source and Target columns in the reduced downloaded dataframe. We then created arcs from the ith node of the Source column to the ith node of the Target column, all weighted with a value equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty graph G\n",
    "G = nx.DiGraph() #Create from networkx an empty directed Graph\n",
    "sources = list(graph['Source']) #List of all nodes that are source in a graph\n",
    "targets = list(graph['Target'])#List of all nodes that are Target in a graph\n",
    "\n",
    "#building G = (V,E)\n",
    "for i in range(len(graph)): #For each row in graph dataset\n",
    "    if sources[i] not in G.nodes: #If source linked to i-th row not in G.nodes\n",
    "        G.add_node(sources[i]) #Add sources[i] like new nodes\n",
    "    if targets[i] not in G.nodes:#If target linked to i-th row not in G.nodes\n",
    "        G.add_node(targets[i]) ##Add targets[i] like new nodes\n",
    "    G.add_edge(sources[i], targets[i], weight=1) #Create edge from i-th sources and i-th targets wit a weight equal o 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many articles are we considering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the total number of nodes in the graph by reporting all the elements of the two columns in the downloaded dataset in the form of a set in order to eliminate any duplication. Once this was done we created a new set given by the union of the previous sets and we calculated its length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_articles(df):\n",
    "    sources = set(df['Source']) #Set of all nodes that are in Source column (Drop the duplicate)\n",
    "    targets = set(df['Target']) #Set of all nodes that are in Target column (Drop the duplicate)\n",
    "    return len(sources.union(targets)) #Compute the len of new set obtained from the union of sources and targets sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = number_articles(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98343"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hyperlinks between pages exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of hyperlinks belonging to the graph is equal to the number of edges present. This value corresponds to the length of the reduced datasaet downloaded as we can consider it as the Edge List of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_hyperlinks(df):\n",
    "    return len(df) #Compute the number of rows in Dataframe containing Edge List because number of hyperlinks equal to number of edges that are equal to number of rows in Graph Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483094"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average number of links in an arbitrary page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the average of links for each node in the graph by calculating the overall sum of the degrees of each single node in the graph and dividing this sum by the number of nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_links(graph):\n",
    "    somma = 0 #We initialized somma variable to zero\n",
    "    for node in graph.nodes: #For each nodes in graph\n",
    "        somma += nx.degree(graph)[node] #We increase the somma variable with the degree of specific node in Graph\n",
    "    return somma/len(graph.nodes) #Average links for each node equal to ratio between the sum and the number of nodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.824674862471147"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_links(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the degree obtained is a direct degree then we can define its density given by the ratio between the number of arcs in the degree and the binomial coefficient of the number of nodes out of 2. The binomial coefficient is multiplied by two being a direct graph and consequently being fundamental the direction of the arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\displaystyle D={\\frac {|E|}{2{\\binom {|V|}{2}}}}={\\frac {|E|}{|V|(|V|-1)}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_density(nodes,edges):\n",
    "    density = edges/(nodes*(nodes-1)) #Graph density equal to ratio between number of edges and twice the binomial coefficient of the number of nodes over 2\n",
    "    return density\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9951571365597335e-05"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_density(nodes,edges) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is a sparse graph whose density D is in the lower range of the density's codomain, or $0 \\leq D < \\frac {1} {2}$. To confirm this we can compare the number of maximum arcs for the graph created with the previously calculated edges value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_edges(nodes,edges): #We want to prove that our graph is sparse to do this we compute the number of max_edges in our Graph and number of real eadges\n",
    "    max_edges = nodes*(nodes-1)\n",
    "    return print(\"The value of max edges is\", max_edges, \"while the value of edges is equals\", edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of max edges is 9671247306 while the value of edges is equals 483094\n"
     ]
    }
   ],
   "source": [
    "comparison_edges(nodes,edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Degree's nodes distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we reported through a plot nodes'degree distribution by defining its degree for each single node within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_degree_dist(graph):\n",
    "    degrees = [graph.degree(node) for node in graph.nodes()] #We compute for each node the degree in our Graph\n",
    "    plt.hist(degrees, bins=[0, 10, 20, 30, 40, 50, 55,60]) #We plot degree's node distribution with an histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_degree_dist(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take in input the starting node and the number of clicks $d$, then starting from initial node we reach all the node at level $k$  with $k\\leq d$ considering the starting node as level 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clicks(graph, start_node, depth):\n",
    "    visited = [start_node] #the starting node is the first visited node\n",
    "    for i in range(depth): #until level d \n",
    "        all_visited =[] # all the nodes visited\n",
    "        for node in visited:\n",
    "            for nbr in graph[node]: #for all the neighborhood  of node\n",
    "                all_visited.append(nbr)  #save the neighborhood in the list \n",
    "        visited = all_visited#saving all the neighborhood     \n",
    "    return set(all_visited) #returning a set to avoid ripetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we can compute all the pages reachable starting from node 95 with 3 clicks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Node</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1595904</th>\n",
       "      <td>Reba McEntire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182721</th>\n",
       "      <td>Ma (album)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061891</th>\n",
       "      <td>Jodie Foster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067523</th>\n",
       "      <td>Valentine (film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060361</th>\n",
       "      <td>Rose McGowan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Pages\n",
       "Node                     \n",
       "1595904     Reba McEntire\n",
       "1182721        Ma (album)\n",
       "1061891      Jodie Foster\n",
       "1067523  Valentine (film)\n",
       "1060361      Rose McGowan"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name.iloc[list(clicks(G, 95, 3))].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5074 Category                                  English_footballers\n",
      "PageList    [22860, 28961, 28979, 29264, 30896, 30900, 319...\n",
      "Name: 0, dtype: object\n",
      "5635 Category                                Year_of_birth_missing\n",
      "PageList    [28411, 72580, 74377, 76045, 76100, 92872, 928...\n",
      "Name: 1, dtype: object\n",
      "4925 Category                          The_Football_League_players\n",
      "PageList    [29573, 33973, 48714, 54184, 72483, 72521, 725...\n",
      "Name: 2, dtype: object\n",
      "5937 Category                                Year_of_death_missing\n",
      "PageList    [29582, 72518, 72528, 72579, 72587, 72591, 726...\n",
      "Name: 3, dtype: object\n",
      "5406 Category                       Association_football_defenders\n",
      "PageList    [48583, 72549, 72617, 72629, 72943, 73090, 731...\n",
      "Name: 4, dtype: object\n",
      "5571 Category                        Association_football_forwards\n",
      "PageList    [72522, 72622, 72696, 72824, 72945, 73060, 731...\n",
      "Name: 5, dtype: object\n",
      "6775 Category                     Association_football_midfielders\n",
      "PageList    [73066, 73120, 73122, 75211, 75212, 75227, 752...\n",
      "Name: 6, dtype: object\n",
      "6431 Category               Place_of_birth_missing_(living_people)\n",
      "PageList    [75430, 603936, 75195, 83446, 86747, 96768, 36...\n",
      "Name: 7, dtype: object\n",
      "6516 Category    Members_of_the_United_Kingdom_Parliament_for_E...\n",
      "PageList    [535217, 536702, 536701, 538562, 538800, 53913...\n",
      "Name: 8, dtype: object\n",
      "6053 Category                            Harvard_University_alumni\n",
      "PageList    [77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...\n",
      "Name: 9, dtype: object\n",
      "6992 Category                           American_television_actors\n",
      "PageList    [143566, 154133, 351585, 419812, 423184, 46937...\n",
      "Name: 10, dtype: object\n",
      "9352 Category                                 American_film_actors\n",
      "PageList    [343966, 633296, 725183, 744132, 1054490, 1054...\n",
      "Name: 11, dtype: object\n",
      "6568 Category                       Major_League_Baseball_pitchers\n",
      "PageList    [79, 24213, 33054, 37167, 53973, 63107, 69823,...\n",
      "Name: 12, dtype: object\n",
      "5834 Category                                         Indian_films\n",
      "PageList    [1308, 29286, 53565, 70274, 70275, 70797, 1233...\n",
      "Name: 13, dtype: object\n",
      "16297 Category                               English-language_films\n",
      "PageList    [581028, 581089, 581149, 581926, 582475, 58656...\n",
      "Name: 14, dtype: object\n",
      "8074 Category                                Black-and-white_films\n",
      "PageList    [581563, 588536, 589066, 589097, 589113, 58914...\n",
      "Name: 15, dtype: object\n",
      "7729 Category                                     Rivers_of_Romani\n",
      "PageList    [72111, 72112, 72113, 72114, 72115, 72116, 721...\n",
      "Name: 16, dtype: object\n",
      "3037 Category                           Asteroids_named_for_people\n",
      "PageList    [13184, 13992, 33990, 33991, 39159, 96568, 965...\n",
      "Name: 17, dtype: object\n",
      "11110 Category                                  Main_Belt_asteroids\n",
      "PageList    [22277, 23539, 29163, 33254, 33401, 33791, 359...\n",
      "Name: 18, dtype: object\n",
      "8077 Category                                       American_films\n",
      "PageList    [153, 1152, 1158, 1267, 2247, 15232, 18901, 19...\n",
      "Name: 19, dtype: object\n",
      "8401 Category                                         Debut_albums\n",
      "PageList    [95, 108, 112, 113, 190, 254, 1413, 1451, 1452...\n",
      "Name: 20, dtype: object\n"
     ]
    }
   ],
   "source": [
    "maximo = 1000000000000000\n",
    "indexa = 0\n",
    "for element, index in new_dat2.iterrows():\n",
    "    print(len(new_dat2.loc[element,\"PageList\"]), index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask for a category as input, then we search for this category in the dataframe containing all the categories and if the category falls, we insert all the pages associated with that category in the set \"nodes\". Then we return either the set nodes or if the category is not present in the dataframe the string \"No Category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>PageList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English_footballers</td>\n",
       "      <td>[22860, 28961, 28979, 29264, 30896, 30900, 319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Year_of_birth_missing</td>\n",
       "      <td>[28411, 72580, 74377, 76045, 76100, 92872, 928...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The_Football_League_players</td>\n",
       "      <td>[29573, 33973, 48714, 54184, 72483, 72521, 725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>[29582, 72518, 72528, 72579, 72587, 72591, 726...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Association_football_defenders</td>\n",
       "      <td>[48583, 72549, 72617, 72629, 72943, 73090, 731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Association_football_forwards</td>\n",
       "      <td>[72522, 72622, 72696, 72824, 72945, 73060, 731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Association_football_midfielders</td>\n",
       "      <td>[73066, 73120, 73122, 75211, 75212, 75227, 752...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "      <td>[75430, 603936, 75195, 83446, 86747, 96768, 36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Members_of_the_United_Kingdom_Parliament_for_E...</td>\n",
       "      <td>[535217, 536702, 536701, 538562, 538800, 53913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Harvard_University_alumni</td>\n",
       "      <td>[77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>[143566, 154133, 351585, 419812, 423184, 46937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>American_film_actors</td>\n",
       "      <td>[343966, 633296, 725183, 744132, 1054490, 1054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Major_League_Baseball_pitchers</td>\n",
       "      <td>[79, 24213, 33054, 37167, 53973, 63107, 69823,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indian_films</td>\n",
       "      <td>[1308, 29286, 53565, 70274, 70275, 70797, 1233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>English-language_films</td>\n",
       "      <td>[581028, 581089, 581149, 581926, 582475, 58656...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Black-and-white_films</td>\n",
       "      <td>[581563, 588536, 589066, 589097, 589113, 58914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rivers_of_Romani</td>\n",
       "      <td>[72111, 72112, 72113, 72114, 72115, 72116, 721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Asteroids_named_for_people</td>\n",
       "      <td>[13184, 13992, 33990, 33991, 39159, 96568, 965...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Main_Belt_asteroids</td>\n",
       "      <td>[22277, 23539, 29163, 33254, 33401, 33791, 359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>American_films</td>\n",
       "      <td>[153, 1152, 1158, 1267, 2247, 15232, 18901, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Debut_albums</td>\n",
       "      <td>[95, 108, 112, 113, 190, 254, 1413, 1451, 1452...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category  \\\n",
       "0                                 English_footballers   \n",
       "1                               Year_of_birth_missing   \n",
       "2                         The_Football_League_players   \n",
       "3                               Year_of_death_missing   \n",
       "4                      Association_football_defenders   \n",
       "5                       Association_football_forwards   \n",
       "6                    Association_football_midfielders   \n",
       "7              Place_of_birth_missing_(living_people)   \n",
       "8   Members_of_the_United_Kingdom_Parliament_for_E...   \n",
       "9                           Harvard_University_alumni   \n",
       "10                         American_television_actors   \n",
       "11                               American_film_actors   \n",
       "12                     Major_League_Baseball_pitchers   \n",
       "13                                       Indian_films   \n",
       "14                             English-language_films   \n",
       "15                              Black-and-white_films   \n",
       "16                                   Rivers_of_Romani   \n",
       "17                         Asteroids_named_for_people   \n",
       "18                                Main_Belt_asteroids   \n",
       "19                                     American_films   \n",
       "20                                       Debut_albums   \n",
       "\n",
       "                                             PageList  \n",
       "0   [22860, 28961, 28979, 29264, 30896, 30900, 319...  \n",
       "1   [28411, 72580, 74377, 76045, 76100, 92872, 928...  \n",
       "2   [29573, 33973, 48714, 54184, 72483, 72521, 725...  \n",
       "3   [29582, 72518, 72528, 72579, 72587, 72591, 726...  \n",
       "4   [48583, 72549, 72617, 72629, 72943, 73090, 731...  \n",
       "5   [72522, 72622, 72696, 72824, 72945, 73060, 731...  \n",
       "6   [73066, 73120, 73122, 75211, 75212, 75227, 752...  \n",
       "7   [75430, 603936, 75195, 83446, 86747, 96768, 36...  \n",
       "8   [535217, 536702, 536701, 538562, 538800, 53913...  \n",
       "9   [77, 1013, 1271, 1663, 1779, 1843, 2212, 3193,...  \n",
       "10  [143566, 154133, 351585, 419812, 423184, 46937...  \n",
       "11  [343966, 633296, 725183, 744132, 1054490, 1054...  \n",
       "12  [79, 24213, 33054, 37167, 53973, 63107, 69823,...  \n",
       "13  [1308, 29286, 53565, 70274, 70275, 70797, 1233...  \n",
       "14  [581028, 581089, 581149, 581926, 582475, 58656...  \n",
       "15  [581563, 588536, 589066, 589097, 589113, 58914...  \n",
       "16  [72111, 72112, 72113, 72114, 72115, 72116, 721...  \n",
       "17  [13184, 13992, 33990, 33991, 39159, 96568, 965...  \n",
       "18  [22277, 23539, 29163, 33254, 33401, 33791, 359...  \n",
       "19  [153, 1152, 1158, 1267, 2247, 15232, 18901, 19...  \n",
       "20  [95, 108, 112, 113, 190, 254, 1413, 1451, 1452...  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(df):\n",
    "    nodes = list() #Create an empty list  \n",
    "    C = input(\"Enter your Category: \") #We ask as input a Category\n",
    "    for i in range(len(df)): #For each row in our dataframe containing category and page lsit\n",
    "        if df.loc[i,'Category'] == C: #If the category that we put in input is equal to category in i-th row\n",
    "            nodes = df.loc[i,\"PageList\"] #We increase our nodes list with all ele\n",
    "    if not nodes: #If nodes list is empty\n",
    "        print(\"No Category\") #Category not appear in our dataframe\n",
    "    else:\n",
    "        return list(map(int, nodes)) #Map every element in list nodes to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Category: American_film_actors\n"
     ]
    }
   ],
   "source": [
    "nodes = category(new_dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[343966,\n",
       " 633296,\n",
       " 725183,\n",
       " 744132,\n",
       " 1054490,\n",
       " 1054933,\n",
       " 1060651,\n",
       " 1064082,\n",
       " 1162421,\n",
       " 1162580,\n",
       " 1261800,\n",
       " 1373051,\n",
       " 1505728,\n",
       " 1659467,\n",
       " 572798,\n",
       " 1235828,\n",
       " 1247269,\n",
       " 1263510,\n",
       " 1268321,\n",
       " 1268555,\n",
       " 1268586,\n",
       " 1269604,\n",
       " 1270563,\n",
       " 1618809,\n",
       " 174,\n",
       " 1088,\n",
       " 1121,\n",
       " 1124,\n",
       " 1133,\n",
       " 1134,\n",
       " 1136,\n",
       " 1139,\n",
       " 1143,\n",
       " 1153,\n",
       " 15182,\n",
       " 24166,\n",
       " 25689,\n",
       " 26503,\n",
       " 30849,\n",
       " 30850,\n",
       " 31888,\n",
       " 32569,\n",
       " 35315,\n",
       " 39423,\n",
       " 53906,\n",
       " 70483,\n",
       " 71575,\n",
       " 71682,\n",
       " 71740,\n",
       " 71744,\n",
       " 94225,\n",
       " 108143,\n",
       " 108208,\n",
       " 108261,\n",
       " 108941,\n",
       " 110138,\n",
       " 110144,\n",
       " 110145,\n",
       " 110162,\n",
       " 110163,\n",
       " 110164,\n",
       " 110175,\n",
       " 110176,\n",
       " 110181,\n",
       " 110182,\n",
       " 110200,\n",
       " 110209,\n",
       " 110407,\n",
       " 110410,\n",
       " 112148,\n",
       " 113195,\n",
       " 113199,\n",
       " 113206,\n",
       " 113216,\n",
       " 113258,\n",
       " 113283,\n",
       " 113343,\n",
       " 132133,\n",
       " 133703,\n",
       " 133704,\n",
       " 134102,\n",
       " 134104,\n",
       " 134109,\n",
       " 134120,\n",
       " 134260,\n",
       " 134272,\n",
       " 134287,\n",
       " 134348,\n",
       " 134361,\n",
       " 134364,\n",
       " 134827,\n",
       " 135016,\n",
       " 135022,\n",
       " 135023,\n",
       " 135035,\n",
       " 135038,\n",
       " 135045,\n",
       " 135136,\n",
       " 135763,\n",
       " 136065,\n",
       " 136176,\n",
       " 136276,\n",
       " 136382,\n",
       " 136480,\n",
       " 136535,\n",
       " 136584,\n",
       " 136595,\n",
       " 136596,\n",
       " 136607,\n",
       " 137138,\n",
       " 137768,\n",
       " 138066,\n",
       " 139762,\n",
       " 139785,\n",
       " 140256,\n",
       " 140338,\n",
       " 140535,\n",
       " 141235,\n",
       " 142167,\n",
       " 143153,\n",
       " 143399,\n",
       " 143525,\n",
       " 143556,\n",
       " 143557,\n",
       " 143560,\n",
       " 143599,\n",
       " 143602,\n",
       " 143615,\n",
       " 143622,\n",
       " 143642,\n",
       " 143644,\n",
       " 143645,\n",
       " 143648,\n",
       " 143651,\n",
       " 144645,\n",
       " 144659,\n",
       " 144743,\n",
       " 146389,\n",
       " 146689,\n",
       " 146730,\n",
       " 146732,\n",
       " 146736,\n",
       " 146743,\n",
       " 146752,\n",
       " 146761,\n",
       " 146763,\n",
       " 146772,\n",
       " 146773,\n",
       " 146791,\n",
       " 146802,\n",
       " 146880,\n",
       " 146894,\n",
       " 147375,\n",
       " 147468,\n",
       " 147475,\n",
       " 147546,\n",
       " 147705,\n",
       " 149361,\n",
       " 151755,\n",
       " 154124,\n",
       " 154125,\n",
       " 154143,\n",
       " 154171,\n",
       " 154212,\n",
       " 154860,\n",
       " 158120,\n",
       " 158124,\n",
       " 158189,\n",
       " 158444,\n",
       " 166605,\n",
       " 166629,\n",
       " 166924,\n",
       " 167342,\n",
       " 200777,\n",
       " 201187,\n",
       " 201631,\n",
       " 205224,\n",
       " 212574,\n",
       " 216566,\n",
       " 230934,\n",
       " 233382,\n",
       " 233542,\n",
       " 234610,\n",
       " 234611,\n",
       " 235155,\n",
       " 235770,\n",
       " 236012,\n",
       " 236912,\n",
       " 237299,\n",
       " 238494,\n",
       " 238738,\n",
       " 242825,\n",
       " 243757,\n",
       " 244033,\n",
       " 247103,\n",
       " 248275,\n",
       " 248348,\n",
       " 248538,\n",
       " 251327,\n",
       " 251851,\n",
       " 251899,\n",
       " 255824,\n",
       " 256014,\n",
       " 256182,\n",
       " 256211,\n",
       " 257527,\n",
       " 257619,\n",
       " 257625,\n",
       " 257990,\n",
       " 258079,\n",
       " 260521,\n",
       " 276461,\n",
       " 289480,\n",
       " 290824,\n",
       " 291549,\n",
       " 292242,\n",
       " 292304,\n",
       " 295538,\n",
       " 295591,\n",
       " 295704,\n",
       " 295791,\n",
       " 295804,\n",
       " 296087,\n",
       " 298877,\n",
       " 299303,\n",
       " 300969,\n",
       " 301032,\n",
       " 306616,\n",
       " 306617,\n",
       " 308678,\n",
       " 308689,\n",
       " 310464,\n",
       " 314922,\n",
       " 315680,\n",
       " 317336,\n",
       " 317959,\n",
       " 319033,\n",
       " 319124,\n",
       " 319154,\n",
       " 320828,\n",
       " 322953,\n",
       " 322995,\n",
       " 323364,\n",
       " 324106,\n",
       " 324161,\n",
       " 324398,\n",
       " 324423,\n",
       " 324436,\n",
       " 325260,\n",
       " 325436,\n",
       " 326618,\n",
       " 326657,\n",
       " 327368,\n",
       " 327369,\n",
       " 327371,\n",
       " 327401,\n",
       " 327403,\n",
       " 327405,\n",
       " 327428,\n",
       " 327429,\n",
       " 327431,\n",
       " 327432,\n",
       " 327434,\n",
       " 327442,\n",
       " 327447,\n",
       " 327457,\n",
       " 327469,\n",
       " 327470,\n",
       " 327492,\n",
       " 331280,\n",
       " 333187,\n",
       " 334273,\n",
       " 334889,\n",
       " 334890,\n",
       " 337791,\n",
       " 338483,\n",
       " 339002,\n",
       " 339892,\n",
       " 340288,\n",
       " 340293,\n",
       " 340294,\n",
       " 340304,\n",
       " 340305,\n",
       " 340309,\n",
       " 340315,\n",
       " 340672,\n",
       " 340702,\n",
       " 341134,\n",
       " 343647,\n",
       " 343962,\n",
       " 344523,\n",
       " 344793,\n",
       " 345094,\n",
       " 345095,\n",
       " 351551,\n",
       " 354066,\n",
       " 354438,\n",
       " 355725,\n",
       " 355899,\n",
       " 355902,\n",
       " 356852,\n",
       " 357269,\n",
       " 367329,\n",
       " 373747,\n",
       " 373749,\n",
       " 375394,\n",
       " 391361,\n",
       " 391369,\n",
       " 392229,\n",
       " 392604,\n",
       " 392662,\n",
       " 393023,\n",
       " 393430,\n",
       " 393465,\n",
       " 400223,\n",
       " 400329,\n",
       " 400363,\n",
       " 400402,\n",
       " 400404,\n",
       " 400406,\n",
       " 400460,\n",
       " 400462,\n",
       " 400471,\n",
       " 400521,\n",
       " 400528,\n",
       " 400575,\n",
       " 400578,\n",
       " 400584,\n",
       " 400585,\n",
       " 400597,\n",
       " 400615,\n",
       " 400616,\n",
       " 400617,\n",
       " 400620,\n",
       " 400622,\n",
       " 400634,\n",
       " 400639,\n",
       " 400669,\n",
       " 400682,\n",
       " 400686,\n",
       " 400691,\n",
       " 400694,\n",
       " 400701,\n",
       " 400723,\n",
       " 400729,\n",
       " 400738,\n",
       " 400739,\n",
       " 400764,\n",
       " 400779,\n",
       " 400784,\n",
       " 400811,\n",
       " 400875,\n",
       " 401882,\n",
       " 401888,\n",
       " 401929,\n",
       " 401939,\n",
       " 401948,\n",
       " 401980,\n",
       " 402210,\n",
       " 402241,\n",
       " 402352,\n",
       " 402368,\n",
       " 402414,\n",
       " 402675,\n",
       " 402683,\n",
       " 402704,\n",
       " 402727,\n",
       " 402730,\n",
       " 402756,\n",
       " 402759,\n",
       " 403237,\n",
       " 403342,\n",
       " 404989,\n",
       " 409997,\n",
       " 419317,\n",
       " 419820,\n",
       " 419829,\n",
       " 419831,\n",
       " 419832,\n",
       " 419843,\n",
       " 419855,\n",
       " 422300,\n",
       " 422782,\n",
       " 422884,\n",
       " 422993,\n",
       " 423148,\n",
       " 426553,\n",
       " 426742,\n",
       " 427487,\n",
       " 427488,\n",
       " 428285,\n",
       " 430110,\n",
       " 430156,\n",
       " 430161,\n",
       " 430226,\n",
       " 430481,\n",
       " 430574,\n",
       " 430910,\n",
       " 430915,\n",
       " 430917,\n",
       " 430918,\n",
       " 430921,\n",
       " 430957,\n",
       " 430964,\n",
       " 431202,\n",
       " 431332,\n",
       " 431340,\n",
       " 431484,\n",
       " 431496,\n",
       " 431581,\n",
       " 432316,\n",
       " 433567,\n",
       " 433574,\n",
       " 438570,\n",
       " 439228,\n",
       " 439326,\n",
       " 439342,\n",
       " 449511,\n",
       " 450891,\n",
       " 451100,\n",
       " 460148,\n",
       " 467072,\n",
       " 468460,\n",
       " 470100,\n",
       " 470564,\n",
       " 475424,\n",
       " 479833,\n",
       " 479877,\n",
       " 479878,\n",
       " 479880,\n",
       " 479900,\n",
       " 480017,\n",
       " 480030,\n",
       " 480043,\n",
       " 480047,\n",
       " 480111,\n",
       " 480119,\n",
       " 480122,\n",
       " 480126,\n",
       " 480130,\n",
       " 480140,\n",
       " 480151,\n",
       " 480154,\n",
       " 480156,\n",
       " 480163,\n",
       " 480178,\n",
       " 480194,\n",
       " 480208,\n",
       " 480223,\n",
       " 481346,\n",
       " 482708,\n",
       " 483098,\n",
       " 493441,\n",
       " 499557,\n",
       " 499910,\n",
       " 499938,\n",
       " 501185,\n",
       " 501200,\n",
       " 501219,\n",
       " 501226,\n",
       " 501236,\n",
       " 501240,\n",
       " 501247,\n",
       " 501265,\n",
       " 501271,\n",
       " 501272,\n",
       " 501281,\n",
       " 501283,\n",
       " 501303,\n",
       " 501308,\n",
       " 501312,\n",
       " 501313,\n",
       " 501322,\n",
       " 501328,\n",
       " 501336,\n",
       " 501373,\n",
       " 501383,\n",
       " 501394,\n",
       " 501426,\n",
       " 501445,\n",
       " 501456,\n",
       " 503050,\n",
       " 517342,\n",
       " 522690,\n",
       " 523402,\n",
       " 524673,\n",
       " 525015,\n",
       " 526470,\n",
       " 526696,\n",
       " 528550,\n",
       " 530694,\n",
       " 531741,\n",
       " 544514,\n",
       " 547484,\n",
       " 556915,\n",
       " 556916,\n",
       " 557122,\n",
       " 560271,\n",
       " 561486,\n",
       " 561829,\n",
       " 568568,\n",
       " 570160,\n",
       " 570863,\n",
       " 571059,\n",
       " 571064,\n",
       " 571131,\n",
       " 571138,\n",
       " 571142,\n",
       " 571151,\n",
       " 571153,\n",
       " 571157,\n",
       " 571159,\n",
       " 571166,\n",
       " 572899,\n",
       " 573261,\n",
       " 573298,\n",
       " 573300,\n",
       " 577644,\n",
       " 592466,\n",
       " 593149,\n",
       " 593207,\n",
       " 594677,\n",
       " 594988,\n",
       " 600178,\n",
       " 601739,\n",
       " 601784,\n",
       " 601788,\n",
       " 601799,\n",
       " 601815,\n",
       " 601995,\n",
       " 602012,\n",
       " 602298,\n",
       " 602344,\n",
       " 603908,\n",
       " 604696,\n",
       " 605542,\n",
       " 605641,\n",
       " 612413,\n",
       " 626826,\n",
       " 626828,\n",
       " 626832,\n",
       " 626838,\n",
       " 626839,\n",
       " 626845,\n",
       " 626846,\n",
       " 626847,\n",
       " 626848,\n",
       " 626853,\n",
       " 626884,\n",
       " 626893,\n",
       " 626908,\n",
       " 626909,\n",
       " 626910,\n",
       " 626913,\n",
       " 626950,\n",
       " 626951,\n",
       " 626957,\n",
       " 626961,\n",
       " 626975,\n",
       " 626977,\n",
       " 626991,\n",
       " 626997,\n",
       " 627021,\n",
       " 627063,\n",
       " 627075,\n",
       " 627077,\n",
       " 627081,\n",
       " 627093,\n",
       " 627094,\n",
       " 627097,\n",
       " 627108,\n",
       " 627116,\n",
       " 627122,\n",
       " 627123,\n",
       " 627152,\n",
       " 627153,\n",
       " 627157,\n",
       " 627158,\n",
       " 627164,\n",
       " 627165,\n",
       " 627170,\n",
       " 627179,\n",
       " 627183,\n",
       " 627192,\n",
       " 627210,\n",
       " 630815,\n",
       " 631482,\n",
       " 631575,\n",
       " 632543,\n",
       " 632557,\n",
       " 632559,\n",
       " 632560,\n",
       " 632561,\n",
       " 632563,\n",
       " 632564,\n",
       " 632565,\n",
       " 632568,\n",
       " 632600,\n",
       " 632630,\n",
       " 632635,\n",
       " 632650,\n",
       " 632655,\n",
       " 632656,\n",
       " 632674,\n",
       " 632729,\n",
       " 632756,\n",
       " 632787,\n",
       " 632789,\n",
       " 632804,\n",
       " 632845,\n",
       " 632885,\n",
       " 632905,\n",
       " 632920,\n",
       " 632922,\n",
       " 632964,\n",
       " 632973,\n",
       " 633016,\n",
       " 633084,\n",
       " 633135,\n",
       " 633173,\n",
       " 633178,\n",
       " 633210,\n",
       " 633214,\n",
       " 633229,\n",
       " 633333,\n",
       " 633431,\n",
       " 633530,\n",
       " 633548,\n",
       " 633557,\n",
       " 633562,\n",
       " 633565,\n",
       " 633574,\n",
       " 633584,\n",
       " 633588,\n",
       " 633589,\n",
       " 633628,\n",
       " 633643,\n",
       " 633649,\n",
       " 633651,\n",
       " 633652,\n",
       " 633656,\n",
       " 633679,\n",
       " 633684,\n",
       " 633697,\n",
       " 633701,\n",
       " 633706,\n",
       " 633712,\n",
       " 633715,\n",
       " 633731,\n",
       " 633771,\n",
       " 633780,\n",
       " 633787,\n",
       " 633793,\n",
       " 633834,\n",
       " 633838,\n",
       " 633948,\n",
       " 633987,\n",
       " 633990,\n",
       " 634015,\n",
       " 634028,\n",
       " 634033,\n",
       " 634034,\n",
       " 634035,\n",
       " 634056,\n",
       " 634057,\n",
       " 634063,\n",
       " 634200,\n",
       " 634212,\n",
       " 634222,\n",
       " 634224,\n",
       " 634229,\n",
       " 634243,\n",
       " 634249,\n",
       " 634253,\n",
       " 634255,\n",
       " 635840,\n",
       " 635841,\n",
       " 635963,\n",
       " 635968,\n",
       " 635984,\n",
       " 635988,\n",
       " 635999,\n",
       " 636008,\n",
       " 639397,\n",
       " 639628,\n",
       " 639777,\n",
       " 639781,\n",
       " 639782,\n",
       " 639817,\n",
       " 652039,\n",
       " 652252,\n",
       " 652406,\n",
       " 655142,\n",
       " 655202,\n",
       " 655206,\n",
       " 655220,\n",
       " 655267,\n",
       " 655392,\n",
       " 655965,\n",
       " 656029,\n",
       " 656040,\n",
       " 656093,\n",
       " 658463,\n",
       " 658519,\n",
       " 659500,\n",
       " 659720,\n",
       " 659729,\n",
       " 659738,\n",
       " 659739,\n",
       " 659742,\n",
       " 659744,\n",
       " 659746,\n",
       " 659775,\n",
       " 659779,\n",
       " 659780,\n",
       " 659789,\n",
       " 659791,\n",
       " 659796,\n",
       " 659797,\n",
       " 659799,\n",
       " 659800,\n",
       " 659805,\n",
       " 659808,\n",
       " 659813,\n",
       " 660111,\n",
       " 665236,\n",
       " 672922,\n",
       " 673792,\n",
       " 675896,\n",
       " 676379,\n",
       " 676498,\n",
       " 677659,\n",
       " 678198,\n",
       " 678366,\n",
       " 678971,\n",
       " 679890,\n",
       " 680717,\n",
       " 687081,\n",
       " 687545,\n",
       " 688051,\n",
       " 688559,\n",
       " 688815,\n",
       " 688831,\n",
       " 688846,\n",
       " 688852,\n",
       " 688885,\n",
       " 688897,\n",
       " 688898,\n",
       " 688905,\n",
       " 688913,\n",
       " 688914,\n",
       " 689011,\n",
       " 689063,\n",
       " 689075,\n",
       " 689076,\n",
       " 689093,\n",
       " 689100,\n",
       " 689117,\n",
       " 689124,\n",
       " 689165,\n",
       " 689220,\n",
       " 689228,\n",
       " 689600,\n",
       " 690007,\n",
       " 690723,\n",
       " 691481,\n",
       " 692410,\n",
       " 694229,\n",
       " 697723,\n",
       " 705573,\n",
       " 705731,\n",
       " 706408,\n",
       " 706957,\n",
       " 708788,\n",
       " 711128,\n",
       " 711129,\n",
       " 711569,\n",
       " 711575,\n",
       " 711592,\n",
       " 711597,\n",
       " 711601,\n",
       " 711653,\n",
       " 711679,\n",
       " 711751,\n",
       " 711795,\n",
       " 711802,\n",
       " 711816,\n",
       " 711861,\n",
       " 711862,\n",
       " 711867,\n",
       " 711878,\n",
       " 711885,\n",
       " 711887,\n",
       " 711895,\n",
       " 711905,\n",
       " 711906,\n",
       " 711907,\n",
       " 711932,\n",
       " 712163,\n",
       " 712175,\n",
       " 712181,\n",
       " 712183,\n",
       " 712184,\n",
       " 712185,\n",
       " 712186,\n",
       " 712187,\n",
       " 712196,\n",
       " 712232,\n",
       " 712234,\n",
       " 712239,\n",
       " 712244,\n",
       " 713075,\n",
       " 719537,\n",
       " 719547,\n",
       " 719569,\n",
       " 719574,\n",
       " 719576,\n",
       " 719711,\n",
       " 721861,\n",
       " 721862,\n",
       " 723389,\n",
       " 723529,\n",
       " 724498,\n",
       " 724531,\n",
       " 724674,\n",
       " 724692,\n",
       " 724693,\n",
       " 724694,\n",
       " 724696,\n",
       " 724699,\n",
       " 724700,\n",
       " 724721,\n",
       " 724725,\n",
       " 724730,\n",
       " 724736,\n",
       " 724739,\n",
       " 724740,\n",
       " 724742,\n",
       " 724743,\n",
       " 724749,\n",
       " 724753,\n",
       " 724759,\n",
       " 724766,\n",
       " 724779,\n",
       " 724784,\n",
       " 724790,\n",
       " 724796,\n",
       " 724811,\n",
       " 724814,\n",
       " 724815,\n",
       " 724817,\n",
       " 724834,\n",
       " 724839,\n",
       " 724841,\n",
       " 724852,\n",
       " 724854,\n",
       " 724888,\n",
       " 724936,\n",
       " 724951,\n",
       " 725025,\n",
       " 725181,\n",
       " 725184,\n",
       " 725199,\n",
       " 725207,\n",
       " 725208,\n",
       " 725228,\n",
       " 725229,\n",
       " 725234,\n",
       " 725236,\n",
       " 725242,\n",
       " 725255,\n",
       " 725257,\n",
       " 725260,\n",
       " 725278,\n",
       " 725301,\n",
       " 725309,\n",
       " 725325,\n",
       " 725362,\n",
       " 725434,\n",
       " 725438,\n",
       " 725475,\n",
       " 725479,\n",
       " 725489,\n",
       " 725501,\n",
       " 725508,\n",
       " 725513,\n",
       " 725563,\n",
       " 725581,\n",
       " 725600,\n",
       " 726141,\n",
       " 726423,\n",
       " 726440,\n",
       " 727716,\n",
       " 730706,\n",
       " 730709,\n",
       " 730710,\n",
       " 730711,\n",
       " 730775,\n",
       " 739014,\n",
       " 739038,\n",
       " 739039,\n",
       " 739040,\n",
       " 739043,\n",
       " 739047,\n",
       " 739064,\n",
       " 739072,\n",
       " 739096,\n",
       " 739170,\n",
       " 739408,\n",
       " 740145,\n",
       " 740211,\n",
       " 740837,\n",
       " 743502,\n",
       " 743539,\n",
       " 743544,\n",
       " 743752,\n",
       " 743753,\n",
       " 743759,\n",
       " 743762,\n",
       " 743763,\n",
       " 743771,\n",
       " 743772,\n",
       " 743774,\n",
       " 743776,\n",
       " 743783,\n",
       " 743785,\n",
       " 743800,\n",
       " 743804,\n",
       " 743806,\n",
       " 743846,\n",
       " 743850,\n",
       " 743853,\n",
       " 743854,\n",
       " 743856,\n",
       " 743864,\n",
       " 743894,\n",
       " 743897,\n",
       " 743906,\n",
       " 743910,\n",
       " 743912,\n",
       " 743915,\n",
       " 743916,\n",
       " 743917,\n",
       " 743918,\n",
       " 743939,\n",
       " 743942,\n",
       " 743943,\n",
       " 743948,\n",
       " 743949,\n",
       " 743953,\n",
       " 743957,\n",
       " 743959,\n",
       " 743962,\n",
       " 743966,\n",
       " 743967,\n",
       " 743984,\n",
       " 743985,\n",
       " 744026,\n",
       " 744087,\n",
       " 744088,\n",
       " 744090,\n",
       " 744110,\n",
       " 744115,\n",
       " 744127,\n",
       " 744143,\n",
       " 744145,\n",
       " 744150,\n",
       " 744170,\n",
       " 744283,\n",
       " 744313,\n",
       " 744318,\n",
       " 744369,\n",
       " 744371,\n",
       " 744377,\n",
       " 744390,\n",
       " 744393,\n",
       " 744451,\n",
       " 744467,\n",
       " 744478,\n",
       " 744753,\n",
       " 744928,\n",
       " 744939,\n",
       " 745271,\n",
       " 745959,\n",
       " 745968,\n",
       " 745969,\n",
       " 745970,\n",
       " 746940,\n",
       " 749601,\n",
       " 749682,\n",
       " 751473,\n",
       " 751539,\n",
       " 751559,\n",
       " 752700,\n",
       " 752969,\n",
       " 752971,\n",
       " 755119,\n",
       " 755762,\n",
       " 756867,\n",
       " 757084,\n",
       " ...]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the list of nodes obtained for the specific category, let's search for the most central node. We use degree-centrality as a measure of centrality. We get the degree-centrality with respect to the entire graph and then we return a dictionary containing as keys the nodes of the specific category and for each node its own centrality, finally we get the dictionary key with the maximum value. The key obtained represents centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator\n",
    "def degree_centrality(G,nodes): \n",
    "    dictionary = {}\n",
    "    tot_graph = nx.degree_centrality(G)\n",
    "    for keys,value in tot_graph.items():\n",
    "        if int(keys) in nodes:\n",
    "            dictionary.update({keys: value})\n",
    "    v = max(dictionary, key=dictionary.get)\n",
    "    return v\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v=degree_centrality(G,nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061501"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Minimum Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_distance(graph,start,goal):\n",
    "    shortest = {}\n",
    "    shortest[start] = 0\n",
    "    visited = set([start])\n",
    "    neighbour = set(graph.neighbors(start))\n",
    "    non_visited = set()\n",
    "    start_distance = 0 \n",
    "    while not all(x in visited for x in goal):\n",
    "        visited = visited.union(neighbour)\n",
    "        while len(neighbour) > 0:\n",
    "            current_vertex = neighbour.pop() \n",
    "            if (current_vertex in goal) and (current_vertex not in shortest.keys()):\n",
    "                shortest[current_vertex] = start_distance + 1 \n",
    "            for element in set(graph.neighbors(current_vertex)):\n",
    "                if element not in visited:\n",
    "                    non_visited.add(element)\n",
    "        \n",
    "        if len(non_visited) == 0:\n",
    "            break\n",
    "        neighbour = non_visited\n",
    "        non_visited = set()\n",
    "        start_distance += 1\n",
    "       \n",
    "    print(\"Partial number Click obtained before the node on which one has arrived has no more neighbors is:\", start_distance)\n",
    "    if nx.is_weakly_connected(G) == False:\n",
    "        return print(\"Not Possible because Graph is not Connected\")\n",
    "    else:\n",
    "        if (nodes - shortest.keys()) != 0:\n",
    "            print(\"From the start nodes\", v, \" we can't reach all pages in nodes\")\n",
    "        else:\n",
    "            print(\"From the start nodes\", v, \"reach all pages in nodes with number of click\", start_distance)\n",
    "   \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial number Click obtained before the node on which one has arrived has no more neighbors is: 22\n",
      "Not Possible because Graph is not Connected\n"
     ]
    }
   ],
   "source": [
    "minimum_distance(G,v,nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RQ4]\n",
    "### Step 1 : costructing the subgraph of C1 and C2\n",
    "To answer question 4 first of all we need to build the subgraph induced by the categories selected, the new graph contains the node of C1 and C2 and all the edges among the nodes of these two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_graph (graph, c1, c2):\n",
    "    \n",
    "    c1_nodes = new_dat[new_dat['Category']== c1]['PageList']     #nodes of c1\n",
    "    lst1 = [int(val) for sublist in c1_nodes for val in sublist] #list of nodes of c1\n",
    "    \n",
    "    c2_nodes = new_dat[new_dat['Category']== c2]['PageList']     #nodes of c2\n",
    "    lst2 = [int(val) for sublist in c2_nodes for val in sublist] #list of nodes of c2\n",
    "    \n",
    "    nodes = lst1+lst2 #list of the nodes of c1 AND c2\n",
    "    \n",
    "    sub_graph = graph.subgraph(nodes).copy() #building the subgraph\n",
    "    \n",
    "    return nx.DiGraph(sub_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first Category: Association_football_forwards\n",
      "Enter the second Category: Association_football_midfielders\n"
     ]
    }
   ],
   "source": [
    "C1 = input(\"Enter the first Category: \")\n",
    "C2 = input(\"Enter the second Category: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sub = sub_graph(G, C1,C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \\nType: DiGraph\\nNumber of nodes: 5266\\nNumber of edges: 7430\\nAverage in degree:   1.4109\\nAverage out degree:   1.4109'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(G_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let v and u two arbitrary pages in the subgraph. What is the minimum set of hyperlinks one can remove to disconnect u and v?\n",
    "This is a <b> min cut </b> problem : we want to find the minimum number of edges to remove in order to disconect two given nodes in the graph. <br>\n",
    "We can solve this problem using two main results:\n",
    "- Max-flow min-cut theorem\n",
    "- Ford_Fulkerson algorithm <br>\n",
    "The theorem asserts that <i>the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut</i> and the algorithm can find the maximum flow so the minimum cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ford_Fulkerson algorithm\n",
    "This algorithm needs the support of a BFS algorithm that returns the path from the node u to node v, if it exists. The idea behind the algorithm is as follows: as long as there is a path from the source (start node) to the sink (end node), with available capacity on all edges in the path, we send flow along one of the paths. Then we find another path, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ford_fulkerson(graph, s, t):\n",
    "    \n",
    "    n = len(graph)\n",
    "    INF = float('Inf')\n",
    "    max_flow = 0\n",
    " \n",
    "    f = [[0 for k in range(n)] for i in range(n)] #flow matrix\n",
    " \n",
    "    while True:  # while there is a path from s to t \n",
    "        # Use BFS to find s-t path \n",
    "        prev = [ -1 for _ in range(n) ] # This array is filled by BFS and to store path\n",
    "        prev[s] = -2 # s marked as visited\n",
    "        q = deque()\n",
    "        q.append(s) # s is visited\n",
    " \n",
    "        while q and prev[t] == -1: #while q is not empty and t is not visited\n",
    "            u = q.popleft() #visit u\n",
    "            for v in range(n):\n",
    "                path_flow = graph[u][v] - f[u][v]  # path_flow = graph(u,v) - f(u,v) for the edge (u,v)\n",
    "                if path_flow > 0 and prev[v] == -1: #if is possible to have a path and v is not visited\n",
    "                    q.append(v) #visit v\n",
    "                    prev[v] = u #store u in index v\n",
    " \n",
    "        if prev[t] == -1:   # if t has not been visited\n",
    "            break\n",
    " \n",
    "        # augment s-t path in residual graph that has been found by BFS\n",
    "        v = t\n",
    "        delta = INF\n",
    "        while True:\n",
    "            u = prev[v]\n",
    "            path_flow = graph[u][v] - f[u][v]\n",
    "            delta = min(delta, path_flow)\n",
    "            v = u\n",
    "            if v == s:\n",
    "                break\n",
    " \n",
    "        v = t\n",
    "        while True:\n",
    "            u = prev[v]\n",
    "            f[u][v] += delta #increase the flow\n",
    "            f[v][u] -= delta #decrease the flow\n",
    "            v = u\n",
    "            if v == s:\n",
    "                break\n",
    " \n",
    "        max_flow += delta\n",
    " \n",
    "    return max_flow #min_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to give an efficient implementation of the algorithm we map the nodes of G_sub into [0,...,len(G_sub.nodes)] and then, we build the adjacency matrix associated at this graph. The function _nx.adjacency_matrix()_ provided by networkx library returns a sparse matrix that we can convert in a numpy matrix using _.toarray()_ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping G_sub into [0,...,len(G_sub.nodes)]\n",
    "mapping = {old_label:new_label for new_label, old_label in enumerate(G_sub.nodes())}\n",
    "H_sub = nx.relabel_nodes(G_sub, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sparse adjacency matrix\n",
    "Adj_Mat_sparse = nx.adjacency_matrix(H_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy adjacency matrix\n",
    "Adj_Mat = Adj_Mat_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nodes u and v of H_sub picked at random\n",
    "u = choice(list(H_sub.nodes))\n",
    "v = choice(list(H_sub.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_cut = ford_fulkerson(Adj_Mat, u, v)\n",
    "min_cut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
